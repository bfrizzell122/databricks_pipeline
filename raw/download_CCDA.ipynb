{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "131ce037-c67c-4987-bd4f-925ba942e502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Requirements\n",
    "1. Download the clinical documents to your local machine. <br/>\n",
    "    - This program was built with expediency in mind as there was a limited time available to download these files prior to expiration.\n",
    "    - I also had not yet setup the AWS/Databricks environment.\n",
    "    - For portability and reliability, I tried to use standard Python libraries.\n",
    "    - For expediency and simplicity, I also used mature and widely-supported Python libraries.\n",
    "\n",
    "#Assumptions\n",
    "1. This program will only be run locally on a single CSV file so these values are hard-coded into the program.\n",
    "    - This can easily be modified to run in the cloud by changing the file path and pointing it to an S3 bucket.\n",
    "2. CCDA files are small and won't require streaming downloads.\n",
    "3. When supporting a Common Data Model like OMOP, the data model is patient-centered rather than event-driven. This led me to organize the files by patient ID first, rather by a more standard partioning model like date.\n",
    "\n",
    "# Improvements\n",
    "1. If the CCDA files to download would continue to be identified within a CSV file at a regular cadence (daily, weekly, etc.), the simplest option would be to use Databricks COPY INTO. This provides idempotency and can be scheduled via Workflows. COPY INTO would identify new CSV \"driver\" files, and then execute the download program to download all CCDA files via the URLs contained within the CSV file. In addition, I would implement threading to execute the function and download the files, including rate-limiting features if necessary. I would also validate the downloads against the CSV driver file to ensure execution was successful.\n",
    "2. If the CCDA files would be directly pushed to an S3 bucket or other cloud storage by an external data provider without the use of the CSV driver file, then Spark Structured Streaming with Auto Loader could be use to process the files incrementally and idempotently. By default, this process uses \"directory listing\" mode which scans all files, compares them to metadata in the checkpoint location, and only processes new files. However, this could be time-consuming for extremely large volumes of files. In that case, I would implement File Notification Mode with AWS SQS which allows Auto Loader to know exactly which files are new without scanning the directory. Depending on the downstream requirements (whether data warehouse updates are needed near real-time or less frequent) you could schedule a continuous job or batch job for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c900cd-a613-4eb5-a598-43e907708402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from urllib import parse, request\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def download_file(url, save_path, file_name):\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL and saves it to the specified path.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        save_path (str): The path where the file should be saved.\n",
    "        file_name (str): The name of the file to be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the save directory if it doesn't exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    full_path = os.path.join(save_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # Download the file\n",
    "        request.urlretrieve(url, full_path)\n",
    "    except Exception as e:\n",
    "        # Normally this would include logging the error, retrying the download, etc.\n",
    "        # For expediency, simply process to the next download\n",
    "        pass\n",
    "\n",
    "\n",
    "# Source file variables\n",
    "url_source_file_path = \".\\SeniorDEAssessment_20250414\\Assessment\\\\\"\n",
    "url_source_file_name = \"ccda_pre_signed_urls.csv\"\n",
    "url_source_full_path = ''.join([url_source_file_path,url_source_file_name])\n",
    "\n",
    "# Save in the Downloads folder in the current directory\n",
    "save_directory = \".\\Downloads\" \n",
    "\n",
    "# Read source file\n",
    "df_urls = pd.read_csv(url_source_full_path)\n",
    "\n",
    "# Parse source file URL into components to easily extract important values: patient_id and file_name\n",
    "df_urls['parsed'] = (df_urls['pre_signed_urls'].apply(lambda x: (parse.urlparse(x)).path)).str.split('/')\n",
    "\n",
    "# Header segments for parsed URL\n",
    "header_list = ['empty','folder1','folder2','patient_id','file_name']\n",
    "\n",
    "# Extract header segments into separate columns\n",
    "df_urls[header_list] = df_urls['parsed'].apply(pd.Series)\n",
    "\n",
    "# Drop unecessary columns\n",
    "df_urls.drop(columns=['parsed','empty','folder1','folder2'], inplace=True)    \n",
    "\n",
    "# Download files\n",
    "df_urls.apply(lambda x: download_file(x.pre_signed_urls, os.path.join(save_directory, x.patient_id), x.file_name), axis=1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "download_CCDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
